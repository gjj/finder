{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"20210311_v0.2\" # no need for file extension, will be used later for file output\n",
    "df = pd.read_csv(\"{}.csv\".format(input_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset used here seems to have some duplicates (see `unique` vs. `count`), and the dataset shows the same UEN with varying entity names - probably because company has been renamed, can confirm this through www.bizfile.gov.sg.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's remove the duplicates, and only take the last item of entities with duplicate entries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=\"Entity Profile UEN\", keep=\"last\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"{}-filtered.csv\".format(input_file), index=False) # re-write to new file, don't include dataframe's index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "We first begin by crawling for the website addresses from existing data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"20210311_v0.2-filtered\" # no need for file extension, will be used later for file output\n",
    "df = pd.read_csv(\"{}.csv\".format(input_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "PROXIES = os.getenv(\"PROXIES\")\n",
    "\n",
    "if not PROXIES:\n",
    "    print(\"No proxies found, please enter them in CSV format in .env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://httpbin.org/ip\"\n",
    "\n",
    "response = requests.get(url)\n",
    "my_ip = response.json()['origin']\n",
    "\n",
    "proxies = {}\n",
    "\n",
    "proxies_string = PROXIES.split(\",\") # split our proxies into array e.g. [\"proxy1.com\", \"proxy2.com\"]\n",
    "\n",
    "for i, p in enumerate(proxies_string):\n",
    "    proxies[i] = {\n",
    "        \"http\": p,\n",
    "        \"https\": p\n",
    "    }\n",
    "\n",
    "proxy_count = len(proxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if proxies are ok\n",
    "# for i in range(proxy_count):\n",
    "#     try:\n",
    "#         response = requests.get(url, proxies=proxies[i])\n",
    "#         ip = response.json()['origin']\n",
    "#         # print(ip)\n",
    "#         # if ip is not my_ip:\n",
    "#     except Exception as e:\n",
    "#         print(\"Proxy {} is down, error={}\".format(i, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_user_agent():\n",
    "    ua_strings = [\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36\"\n",
    "        \"Mozilla/5.0 (iPhone; CPU iPhone OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/11.1.2 Safari/605.1.15\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 11_2_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1 Safari/605.1.15\",\n",
    "        \"Mozilla/5.0 (iPad; CPU OS 14_4 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) CriOS/87.0.4280.77 Mobile/15E148 Safari/604.1\",\n",
    "        \"Mozilla/5.0 (Linux; Android 10; SM-A205U) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Mobile Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Linux; Android 10; SM-N960U) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Mobile Safari/537.36\"\n",
    "    ]\n",
    " \n",
    "    return random.choice(ua_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning job with id=22 of size=23231\n",
      "Searching for id=22 53407808K STUDIO NAO\n",
      "    [SUCCESS] Found company info for id=22 on retry=0 FASHION (INCLUDING ACCESSORIES) DESIGN SERVICES  FASHION (INCLUDING ACCESSORIES) DESIGN SERVICES\n",
      "Searching for id=23 53407813C TENET ASSURANCE\n",
      "    [ERROR] Failed at id=23 retry=0 error=list index out of range, retrying...\n"
     ]
    }
   ],
   "source": [
    "# Target URL to scrape\n",
    "\n",
    "url = \"http://3.0.205.74/search-results?target={\\\"value\\\":\\\"Registration Number\\\",\\\"label\\\":\\\"Registration Number\\\",\\\"searchTarget\\\":\\\"registrationNumber\\\"}&value=\"\n",
    "output_file = \"{}-result.csv\".format(input_file)\n",
    "size = len(df) # Size of dataset\n",
    "\n",
    "start = 22\n",
    "print(\"Beginning job with id={} of size={}\".format(start, size))\n",
    "\n",
    "max_tries = 15\n",
    "\n",
    "for i in range(start, size):\n",
    "    entity = df.loc[i] # current row record\n",
    "    entity_uen = entity[1] # entity UEN\n",
    "    entity_name = entity[2] # entity name\n",
    "    \n",
    "    # Prepare the CSV to write out\n",
    "    formatted_csv_row = \"\\\"{}\\\",\\\"{}\\\",\\\"{}\\\",\\\"{}\\\"\".format(\n",
    "        entity[0],\n",
    "        entity[1],\n",
    "        entity[2],\n",
    "        entity[3]\n",
    "    )\n",
    "\n",
    "    # Accessing the webpage\n",
    "    print(\"Searching for id={} {} {}\".format(i, entity_uen, entity_name))\n",
    "    \n",
    "    for j in range(max_tries):\n",
    "        try:\n",
    "            random_stuff = \"&ra={}&utm_source=facebook&utm_medium=facebook&utm_campaign=\".format(str(random.random()))\n",
    "            url_new = url + entity_uen + random_stuff\n",
    "\n",
    "            # Setup User Agent headers, attempt to imitate a \"browser-like\" request to the webpage\n",
    "            headers = requests.utils.default_headers()\n",
    "            headers.update({\n",
    "                \"User-Agent\": get_random_user_agent()\n",
    "            })\n",
    "\n",
    "            # Pick a random rotating proxy\n",
    "            id = random.randint(0, proxy_count)\n",
    "            \n",
    "            # Now, we query the target URL using a random proxy\n",
    "            resp = requests.get(url_new, headers=headers, proxies=proxies[id])\n",
    "            \n",
    "            # Use BeautifulSoup to parse the HTML\n",
    "            soup = bs4.BeautifulSoup(resp.text, \"html.parser\")\n",
    "            # print(soup)\n",
    "            \n",
    "            # Now look for the specific elements\n",
    "            columns = soup.find_all(\"div\", {\"class\": \"rt-td table-cell\"})\n",
    "            \n",
    "            description = columns[0].get_text()\n",
    "            website = columns[3].get_text()\n",
    "            ssic = columns[5].get_text()\n",
    "            \n",
    "            # Then append SGPGrid's data...\n",
    "            formatted_csv_row += \",\\\"{}\\\",\\\"{}\\\",\\\"{}\\\"\".format(description, website, ssic)\n",
    "\n",
    "            # Finally we write out to file by appending\n",
    "            f = open(output_file, \"a\")\n",
    "            f.write(formatted_csv_row + \"\\n\")\n",
    "            f.close()\n",
    "            \n",
    "            print(\"    [SUCCESS] Found company info for id={} on retry={}\".format(i, j), description, website, ssic)\n",
    "    \n",
    "            break # next for loop\n",
    "        except Exception as e:\n",
    "            print(\"    [ERROR] Failed at id={} retry={} error={}, retrying...\".format(i, j, e))\n",
    "            \n",
    "            if j == max_tries - 1:\n",
    "                f = open(\"{}-failed.csv\".format(input_file), \"a\")\n",
    "                f.write(formatted_csv_row)\n",
    "                f.close()\n",
    "            continue # retry\n",
    "\n",
    "    time.sleep(random.randint(1, 8)) # Randomise the waiting time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Codes\n",
    "\n",
    "The following code attempts to:\n",
    "\n",
    "- crawl Google Search for the first 10 results\n",
    "- then it grabs the URLs so that we can crawl them again for the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Target URL to scrape\n",
    "\n",
    "# url = \"https://www.google.com/search?q={}&sourceid=chrome&ie=UTF-8\"\n",
    "# output_file = \"{}-result.csv\".format(input_file)\n",
    "# size = len(df) # Size of dataset\n",
    "\n",
    "# max_tries = 15\n",
    "\n",
    "# for i in range(0, size):\n",
    "#     entity = df.loc[i] # current row record\n",
    "#     entity_uen = entity[1] # entity UEN\n",
    "#     entity_name = entity[2] # entity name\n",
    "    \n",
    "#     # Randomise search terms\n",
    "#     search_terms = [\n",
    "#         \"{} singapore website\".format(entity_name),\n",
    "#         \"{} sg website\".format(entity_name)\n",
    "#     ]\n",
    "    \n",
    "#     search_terms_count = len(search_terms)\n",
    "    \n",
    "#     # Prepare the CSV to write out\n",
    "#     formatted_csv_row = \"\\\"{}\\\",\\\"{}\\\",\\\"{}\\\",\\\"{}\\\"\".format(\n",
    "#         entity[0],\n",
    "#         entity[1],\n",
    "#         entity[2],\n",
    "#         entity[3]\n",
    "#     )\n",
    "\n",
    "#     # Accessing the webpage\n",
    "#     print(\"Searching for id={} {} {}\".format(i, entity_uen, entity_name))\n",
    "    \n",
    "#     for j in range(max_tries):\n",
    "#         try:\n",
    "#             id = random.randint(0, search_terms_count)\n",
    "#             search_term = search_terms[id]\n",
    "#             url_new = url.format(search_term)\n",
    "            \n",
    "#             # Setup User Agent headers, attempt to imitate a \"browser-like\" request to the webpage\n",
    "#             headers = requests.utils.default_headers()\n",
    "#             headers.update({\n",
    "#                 'User-Agent': get_random_user_agent()\n",
    "#             })\n",
    "\n",
    "#             # Pick a random rotating proxy\n",
    "#             id = random.randint(0, proxy_count)\n",
    "            \n",
    "#             # Now, we query the target URL using a random proxy\n",
    "#             resp = requests.get(url_new, headers=headers, proxies=proxies[id])\n",
    "            \n",
    "#             # Use BeautifulSoup to parse the HTML\n",
    "#             soup = bs4.BeautifulSoup(resp.text, \"html.parser\")\n",
    "#             print(soup)\n",
    "            \n",
    "#             # Now look for the specific elements\n",
    "#             headers = soup.find_all(\"h3\")\n",
    "#             links = soup.find_all(href=re.compile(r'\\/url\\?q=')) # pick top 10 search results & its link\n",
    "            \n",
    "#             for i in range(10):\n",
    "#                 google_url = links[i].get(\"href\").strip()\n",
    "#                 parsed_url = parse_qs(google_url)\n",
    "#                 formatted_csv_row += \",{}\".format(parsed_url[\"/url?q\"][0])\n",
    "            \n",
    "#             # Finally we write out to file by appending\n",
    "#             f = open(output_file, \"a\")\n",
    "#             f.write(formatted_csv_row + \"\\n\")\n",
    "#             f.close()\n",
    "            \n",
    "#             print(\"    [SUCCESS] Wrote to file for id={} on retry={}\".format(i, j))\n",
    "    \n",
    "#             break # next for loop\n",
    "#         except Exception as e:\n",
    "#             print(\"    [ERROR] Failed at id={} retry={} error={}, retrying...\".format(i, j, e))\n",
    "            \n",
    "#             if j == max_tries - 1:\n",
    "#                 f = open(\"{}-failed.csv\".format(input_file), \"a\")\n",
    "#                 f.write(formatted_csv_row)\n",
    "#                 f.close()\n",
    "#             continue # retry\n",
    "\n",
    "#     time.sleep(random.randint(1, 3)) # Randomise the waiting time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
