{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install cloudscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"20210311_v0.2\" # no need for file extension, will be used later for file output\n",
    "df = pd.read_csv(\"{}.csv\".format(input_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset used here seems to have some duplicates (see `unique` vs. `count`), and the dataset shows the same UEN with varying entity names - probably because company has been renamed, can confirm this through www.bizfile.gov.sg.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity Registration Date</th>\n",
       "      <th>Entity Profile UEN</th>\n",
       "      <th>Entity Name</th>\n",
       "      <th>Primary Section Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>24245</td>\n",
       "      <td>24245</td>\n",
       "      <td>24245</td>\n",
       "      <td>24245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>362</td>\n",
       "      <td>23231</td>\n",
       "      <td>23899</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>2020-09-16</td>\n",
       "      <td>202028206K</td>\n",
       "      <td>ASIA PAAS HOLDINGS PTE. LTD.</td>\n",
       "      <td>PROFESSIONAL, SCIENTIFIC AND TECHNICAL ACTIVITIES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>198</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>10638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Entity Registration Date Entity Profile UEN  \\\n",
       "count                     24245              24245   \n",
       "unique                      362              23231   \n",
       "top                  2020-09-16         202028206K   \n",
       "freq                        198                  5   \n",
       "\n",
       "                         Entity Name  \\\n",
       "count                          24245   \n",
       "unique                         23899   \n",
       "top     ASIA PAAS HOLDINGS PTE. LTD.   \n",
       "freq                               3   \n",
       "\n",
       "                              Primary Section Description  \n",
       "count                                               24245  \n",
       "unique                                                  3  \n",
       "top     PROFESSIONAL, SCIENTIFIC AND TECHNICAL ACTIVITIES  \n",
       "freq                                                10638  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity Registration Date</th>\n",
       "      <th>Entity Profile UEN</th>\n",
       "      <th>Entity Name</th>\n",
       "      <th>Primary Section Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>53407676M</td>\n",
       "      <td>GRAVITY FILM</td>\n",
       "      <td>INFORMATION AND COMMUNICATIONS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>53407679C</td>\n",
       "      <td>SMARTMOUTH</td>\n",
       "      <td>PROFESSIONAL, SCIENTIFIC AND TECHNICAL ACTIVITIES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>53407682C</td>\n",
       "      <td>INNOVIC TECHNOLOGY</td>\n",
       "      <td>PROFESSIONAL, SCIENTIFIC AND TECHNICAL ACTIVITIES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>53407694K</td>\n",
       "      <td>SUPREM9 SOLUTIONS</td>\n",
       "      <td>PROFESSIONAL, SCIENTIFIC AND TECHNICAL ACTIVITIES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>53407706D</td>\n",
       "      <td>THE LOVERS</td>\n",
       "      <td>PROFESSIONAL, SCIENTIFIC AND TECHNICAL ACTIVITIES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24240</th>\n",
       "      <td>2020-12-17</td>\n",
       "      <td>T20VC0183A</td>\n",
       "      <td>SEAVI ADVENT EQUITY VII FUND VCC</td>\n",
       "      <td>FINANCIAL AND INSURANCE ACTIVITIES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24241</th>\n",
       "      <td>2020-12-23</td>\n",
       "      <td>T20VC0185D</td>\n",
       "      <td>WELLINGTON MANAGEMENT FUNDS (SINGAPORE) VCC</td>\n",
       "      <td>FINANCIAL AND INSURANCE ACTIVITIES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24242</th>\n",
       "      <td>2020-12-23</td>\n",
       "      <td>T20VC0187G</td>\n",
       "      <td>PENCO CAPITAL VCC</td>\n",
       "      <td>FINANCIAL AND INSURANCE ACTIVITIES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24243</th>\n",
       "      <td>2020-12-23</td>\n",
       "      <td>T20VC0190G</td>\n",
       "      <td>RAINMAKING VENTURES (S) VCC</td>\n",
       "      <td>FINANCIAL AND INSURANCE ACTIVITIES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24244</th>\n",
       "      <td>2020-12-24</td>\n",
       "      <td>T20VC0192K</td>\n",
       "      <td>HERITAS CAPITAL VCC</td>\n",
       "      <td>FINANCIAL AND INSURANCE ACTIVITIES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24245 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Entity Registration Date Entity Profile UEN  \\\n",
       "0                   2020-01-01          53407676M   \n",
       "1                   2020-01-01          53407679C   \n",
       "2                   2020-01-01          53407682C   \n",
       "3                   2020-01-01          53407694K   \n",
       "4                   2020-01-01          53407706D   \n",
       "...                        ...                ...   \n",
       "24240               2020-12-17         T20VC0183A   \n",
       "24241               2020-12-23         T20VC0185D   \n",
       "24242               2020-12-23         T20VC0187G   \n",
       "24243               2020-12-23         T20VC0190G   \n",
       "24244               2020-12-24         T20VC0192K   \n",
       "\n",
       "                                       Entity Name  \\\n",
       "0                                     GRAVITY FILM   \n",
       "1                                       SMARTMOUTH   \n",
       "2                               INNOVIC TECHNOLOGY   \n",
       "3                                SUPREM9 SOLUTIONS   \n",
       "4                                       THE LOVERS   \n",
       "...                                            ...   \n",
       "24240             SEAVI ADVENT EQUITY VII FUND VCC   \n",
       "24241  WELLINGTON MANAGEMENT FUNDS (SINGAPORE) VCC   \n",
       "24242                            PENCO CAPITAL VCC   \n",
       "24243                  RAINMAKING VENTURES (S) VCC   \n",
       "24244                          HERITAS CAPITAL VCC   \n",
       "\n",
       "                             Primary Section Description  \n",
       "0                         INFORMATION AND COMMUNICATIONS  \n",
       "1      PROFESSIONAL, SCIENTIFIC AND TECHNICAL ACTIVITIES  \n",
       "2      PROFESSIONAL, SCIENTIFIC AND TECHNICAL ACTIVITIES  \n",
       "3      PROFESSIONAL, SCIENTIFIC AND TECHNICAL ACTIVITIES  \n",
       "4      PROFESSIONAL, SCIENTIFIC AND TECHNICAL ACTIVITIES  \n",
       "...                                                  ...  \n",
       "24240                 FINANCIAL AND INSURANCE ACTIVITIES  \n",
       "24241                 FINANCIAL AND INSURANCE ACTIVITIES  \n",
       "24242                 FINANCIAL AND INSURANCE ACTIVITIES  \n",
       "24243                 FINANCIAL AND INSURANCE ACTIVITIES  \n",
       "24244                 FINANCIAL AND INSURANCE ACTIVITIES  \n",
       "\n",
       "[24245 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's remove the duplicates, and only take the last item of entities with duplicate entries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity Registration Date</th>\n",
       "      <th>Entity Profile UEN</th>\n",
       "      <th>Entity Name</th>\n",
       "      <th>Primary Section Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>53407676M</td>\n",
       "      <td>GRAVITY FILM</td>\n",
       "      <td>INFORMATION AND COMMUNICATIONS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>53407679C</td>\n",
       "      <td>SMARTMOUTH</td>\n",
       "      <td>PROFESSIONAL, SCIENTIFIC AND TECHNICAL ACTIVITIES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>53407682C</td>\n",
       "      <td>INNOVIC TECHNOLOGY</td>\n",
       "      <td>PROFESSIONAL, SCIENTIFIC AND TECHNICAL ACTIVITIES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>53407694K</td>\n",
       "      <td>SUPREM9 SOLUTIONS</td>\n",
       "      <td>PROFESSIONAL, SCIENTIFIC AND TECHNICAL ACTIVITIES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>53407706D</td>\n",
       "      <td>THE LOVERS</td>\n",
       "      <td>PROFESSIONAL, SCIENTIFIC AND TECHNICAL ACTIVITIES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24240</th>\n",
       "      <td>2020-12-17</td>\n",
       "      <td>T20VC0183A</td>\n",
       "      <td>SEAVI ADVENT EQUITY VII FUND VCC</td>\n",
       "      <td>FINANCIAL AND INSURANCE ACTIVITIES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24241</th>\n",
       "      <td>2020-12-23</td>\n",
       "      <td>T20VC0185D</td>\n",
       "      <td>WELLINGTON MANAGEMENT FUNDS (SINGAPORE) VCC</td>\n",
       "      <td>FINANCIAL AND INSURANCE ACTIVITIES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24242</th>\n",
       "      <td>2020-12-23</td>\n",
       "      <td>T20VC0187G</td>\n",
       "      <td>PENCO CAPITAL VCC</td>\n",
       "      <td>FINANCIAL AND INSURANCE ACTIVITIES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24243</th>\n",
       "      <td>2020-12-23</td>\n",
       "      <td>T20VC0190G</td>\n",
       "      <td>RAINMAKING VENTURES (S) VCC</td>\n",
       "      <td>FINANCIAL AND INSURANCE ACTIVITIES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24244</th>\n",
       "      <td>2020-12-24</td>\n",
       "      <td>T20VC0192K</td>\n",
       "      <td>HERITAS CAPITAL VCC</td>\n",
       "      <td>FINANCIAL AND INSURANCE ACTIVITIES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23231 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Entity Registration Date Entity Profile UEN  \\\n",
       "0                   2020-01-01          53407676M   \n",
       "1                   2020-01-01          53407679C   \n",
       "2                   2020-01-01          53407682C   \n",
       "3                   2020-01-01          53407694K   \n",
       "4                   2020-01-01          53407706D   \n",
       "...                        ...                ...   \n",
       "24240               2020-12-17         T20VC0183A   \n",
       "24241               2020-12-23         T20VC0185D   \n",
       "24242               2020-12-23         T20VC0187G   \n",
       "24243               2020-12-23         T20VC0190G   \n",
       "24244               2020-12-24         T20VC0192K   \n",
       "\n",
       "                                       Entity Name  \\\n",
       "0                                     GRAVITY FILM   \n",
       "1                                       SMARTMOUTH   \n",
       "2                               INNOVIC TECHNOLOGY   \n",
       "3                                SUPREM9 SOLUTIONS   \n",
       "4                                       THE LOVERS   \n",
       "...                                            ...   \n",
       "24240             SEAVI ADVENT EQUITY VII FUND VCC   \n",
       "24241  WELLINGTON MANAGEMENT FUNDS (SINGAPORE) VCC   \n",
       "24242                            PENCO CAPITAL VCC   \n",
       "24243                  RAINMAKING VENTURES (S) VCC   \n",
       "24244                          HERITAS CAPITAL VCC   \n",
       "\n",
       "                             Primary Section Description  \n",
       "0                         INFORMATION AND COMMUNICATIONS  \n",
       "1      PROFESSIONAL, SCIENTIFIC AND TECHNICAL ACTIVITIES  \n",
       "2      PROFESSIONAL, SCIENTIFIC AND TECHNICAL ACTIVITIES  \n",
       "3      PROFESSIONAL, SCIENTIFIC AND TECHNICAL ACTIVITIES  \n",
       "4      PROFESSIONAL, SCIENTIFIC AND TECHNICAL ACTIVITIES  \n",
       "...                                                  ...  \n",
       "24240                 FINANCIAL AND INSURANCE ACTIVITIES  \n",
       "24241                 FINANCIAL AND INSURANCE ACTIVITIES  \n",
       "24242                 FINANCIAL AND INSURANCE ACTIVITIES  \n",
       "24243                 FINANCIAL AND INSURANCE ACTIVITIES  \n",
       "24244                 FINANCIAL AND INSURANCE ACTIVITIES  \n",
       "\n",
       "[23231 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates(subset=\"Entity Profile UEN\", keep=\"last\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"{}-filtered.csv\".format(input_file), index=False) # re-write to new file, don't include dataframe's index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "We first begin by crawling for the website addresses from existing data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import unittest\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"20210311_v0.2-filtered\" # no need for file extension, will be used later for file output\n",
    "df = pd.read_csv(\"{}.csv\".format(input_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "PROXIES = os.getenv(\"PROXIES\")\n",
    "\n",
    "if not PROXIES:\n",
    "    print(\"No proxies found, please enter them in CSV format in .env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://httpbin.org/ip\"\n",
    "\n",
    "response = requests.get(url)\n",
    "my_ip = response.json()['origin']\n",
    "\n",
    "proxies = {}\n",
    "\n",
    "proxies_string = PROXIES.split(\",\") # split our proxies into array e.g. [\"proxy1.com\", \"proxy2.com\"]\n",
    "\n",
    "for i, p in enumerate(proxies_string):\n",
    "    proxies[i] = {\n",
    "        \"http\": p,\n",
    "        \"https\": p\n",
    "    }\n",
    "\n",
    "proxy_count = len(proxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if proxies are ok\n",
    "# for i in range(proxy_count):\n",
    "#     try:\n",
    "#         response = requests.get(url, proxies=proxies[i])\n",
    "#         ip = response.json()['origin']\n",
    "#         # print(ip)\n",
    "#         # if ip is not my_ip:\n",
    "#     except Exception as e:\n",
    "#         print(\"Proxy {} is down, error={}\".format(i, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_user_agent():\n",
    "    ua_strings = [\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36\"\n",
    "        \"Mozilla/5.0 (iPhone; CPU iPhone OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/11.1.2 Safari/605.1.15\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 11_2_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1 Safari/605.1.15\",\n",
    "        \"Mozilla/5.0 (iPad; CPU OS 14_4 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) CriOS/87.0.4280.77 Mobile/15E148 Safari/604.1\",\n",
    "        \"Mozilla/5.0 (Linux; Android 10; SM-A205U) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Mobile Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Linux; Android 10; SM-N960U) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Mobile Safari/537.36\"\n",
    "    ]\n",
    " \n",
    "    return random.choice(ua_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_brackets (__main__.TestEntityUrlSlug) ... ok\n",
      "test_brackets_2 (__main__.TestEntityUrlSlug) ... ok\n",
      "test_brackets_3 (__main__.TestEntityUrlSlug) ... ok\n",
      "test_dashes_and_ampersands (__main__.TestEntityUrlSlug) ... ok\n",
      "test_period_in_entity_name (__main__.TestEntityUrlSlug) ... ok\n",
      "test_period_in_entity_name_2 (__main__.TestEntityUrlSlug) ... ok\n",
      "test_pte_limited (__main__.TestEntityUrlSlug) ... ok\n",
      "test_special_characters (__main__.TestEntityUrlSlug) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 8 tests in 0.013s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x262a2c0fb50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_entity_url_slug(entity_name):\n",
    "    entity_url_slug = re.sub(\"[^a-zA-Z0-9 ]\", \"\", entity_name.lower())\n",
    "    entity_url_slug = re.sub(\" +\", \" \", entity_url_slug).replace(\" \", \"-\")\n",
    "    return entity_url_slug\n",
    "\n",
    "class TestEntityUrlSlug(unittest.TestCase):\n",
    "    \n",
    "    def test_brackets(self):\n",
    "        self.assertEqual(get_entity_url_slug(\"POLY GLOBAL COMMERCIAL (AU) HOLDINGS PTE. LTD.\"), \"poly-global-commercial-au-holdings-pte-ltd\")\n",
    "    \n",
    "    def test_brackets_2(self):\n",
    "        self.assertEqual(get_entity_url_slug(\"ES SOLUTIONS (S) PTE. LTD.\"), \"es-solutions-s-pte-ltd\")\n",
    "    \n",
    "    def test_brackets_3(self):\n",
    "        self.assertEqual(get_entity_url_slug(\"JEM(S) WATERPLUS PTE. LTD.\"), \"jems-waterplus-pte-ltd\")\n",
    "        \n",
    "    def test_special_characters(self):\n",
    "        self.assertEqual(get_entity_url_slug(\"G+ PTE. LTD.\"), \"g-pte-ltd\")\n",
    "        \n",
    "    def test_period_in_entity_name(self):\n",
    "        self.assertEqual(get_entity_url_slug(\"IK.SG PTE. LTD.\"), \"iksg-pte-ltd\")\n",
    "        \n",
    "    def test_period_in_entity_name_2(self):\n",
    "        self.assertEqual(get_entity_url_slug(\"JIUZHANG TECHNOLOGIES (S.E.A.) PTE. LTD.\"), \"jiuzhang-technologies-sea-pte-ltd\")\n",
    "    \n",
    "    def test_dashes_and_ampersands(self):\n",
    "        self.assertEqual(get_entity_url_slug(\"PRO-FIN CONSULT & COMPANY PTE. LTD.\"), \"profin-consult-company-pte-ltd\")\n",
    "        \n",
    "    def test_pte_limited(self):\n",
    "        self.assertEqual(get_entity_url_slug(\"ASIA OPPORTUNITIES (SINGAPORE) PTE. LIMITED\"), \"asia-opportunities-singapore-pte-limited\")\n",
    "    \n",
    "    def test_brackets_2(self):\n",
    "        self.assertEqual(get_entity_url_slug(\"ES SOLUTIONS (S) PTE. LTD.\"), \"es-solutions-s-pte-ltd\")\n",
    "    \n",
    "        \n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning job with id=79 of size=23231\n",
      "Searching for id=79 53408070K ALPYX\n",
      "https://sgpgrid.com/company-details/alpyx?ra=0.7510683232325807&utm_source=facebook&utm_medium=facebook&utm_campaign=\n",
      "    [ERROR] Failed at id=79 retry=0 error=Detected a Cloudflare version 2 Captcha challenge, This feature is not available in the opensource (free) version. using proxy id=12, retrying...\n",
      "https://sgpgrid.com/company-details/alpyx?ra=0.8674663678430043&utm_source=facebook&utm_medium=facebook&utm_campaign=\n",
      "    [ERROR] Failed at id=79 retry=1 error=Detected a Cloudflare version 2 Captcha challenge, This feature is not available in the opensource (free) version. using proxy id=10, retrying...\n",
      "https://sgpgrid.com/company-details/alpyx?ra=0.22067073318082442&utm_source=facebook&utm_medium=facebook&utm_campaign=\n",
      "    [ERROR] Failed at id=79 retry=2 error=Detected a Cloudflare version 2 Captcha challenge, This feature is not available in the opensource (free) version. using proxy id=5, retrying...\n",
      "https://sgpgrid.com/company-details/alpyx?ra=0.9825560033993407&utm_source=facebook&utm_medium=facebook&utm_campaign=\n",
      "    [ERROR] Failed at id=79 retry=3 error=Detected a Cloudflare version 2 Captcha challenge, This feature is not available in the opensource (free) version. using proxy id=5, retrying...\n",
      "https://sgpgrid.com/company-details/alpyx?ra=0.2778031351662712&utm_source=facebook&utm_medium=facebook&utm_campaign=\n",
      "    [ERROR] Failed at id=79 retry=4 error=Detected a Cloudflare version 2 Captcha challenge, This feature is not available in the opensource (free) version. using proxy id=1, retrying...\n",
      "https://sgpgrid.com/company-details/alpyx?ra=0.563618643797419&utm_source=facebook&utm_medium=facebook&utm_campaign=\n",
      "    [ERROR] Failed at id=79 retry=5 error=Detected a Cloudflare version 2 Captcha challenge, This feature is not available in the opensource (free) version. using proxy id=2, retrying...\n",
      "https://sgpgrid.com/company-details/alpyx?ra=0.32508947365651464&utm_source=facebook&utm_medium=facebook&utm_campaign=\n",
      "    [ERROR] Failed at id=79 retry=6 error=Detected a Cloudflare version 2 Captcha challenge, This feature is not available in the opensource (free) version. using proxy id=5, retrying...\n",
      "https://sgpgrid.com/company-details/alpyx?ra=0.8763445408139886&utm_source=facebook&utm_medium=facebook&utm_campaign=\n",
      "    [ERROR] Failed at id=79 retry=7 error=Detected a Cloudflare version 2 Captcha challenge, This feature is not available in the opensource (free) version. using proxy id=0, retrying...\n",
      "https://sgpgrid.com/company-details/alpyx?ra=0.31561307653071924&utm_source=facebook&utm_medium=facebook&utm_campaign=\n",
      "    [ERROR] Failed at id=79 retry=8 error=Detected a Cloudflare version 2 Captcha challenge, This feature is not available in the opensource (free) version. using proxy id=2, retrying...\n",
      "https://sgpgrid.com/company-details/alpyx?ra=0.7722183586767887&utm_source=facebook&utm_medium=facebook&utm_campaign=\n",
      "    [ERROR] Failed at id=79 retry=9 error=Detected a Cloudflare version 2 Captcha challenge, This feature is not available in the opensource (free) version. using proxy id=2, retrying...\n",
      "https://sgpgrid.com/company-details/alpyx?ra=0.5663815918456351&utm_source=facebook&utm_medium=facebook&utm_campaign=\n",
      "    [ERROR] Failed at id=79 retry=10 error=Detected a Cloudflare version 2 Captcha challenge, This feature is not available in the opensource (free) version. using proxy id=3, retrying...\n",
      "https://sgpgrid.com/company-details/alpyx?ra=0.9424115073347143&utm_source=facebook&utm_medium=facebook&utm_campaign=\n",
      "    [ERROR] Failed at id=79 retry=11 error=Detected a Cloudflare version 2 Captcha challenge, This feature is not available in the opensource (free) version. using proxy id=10, retrying...\n",
      "https://sgpgrid.com/company-details/alpyx?ra=0.33394383928635385&utm_source=facebook&utm_medium=facebook&utm_campaign=\n",
      "    [ERROR] Failed at id=79 retry=12 error=Detected a Cloudflare version 2 Captcha challenge, This feature is not available in the opensource (free) version. using proxy id=7, retrying...\n",
      "https://sgpgrid.com/company-details/alpyx?ra=0.4883114503236954&utm_source=facebook&utm_medium=facebook&utm_campaign=\n",
      "    [ERROR] Failed at id=79 retry=13 error=Detected a Cloudflare version 2 Captcha challenge, This feature is not available in the opensource (free) version. using proxy id=12, retrying...\n",
      "https://sgpgrid.com/company-details/alpyx?ra=0.3227591664921501&utm_source=facebook&utm_medium=facebook&utm_campaign=\n",
      "    [ERROR] Failed at id=79 retry=14 error=Detected a Cloudflare version 2 Captcha challenge, This feature is not available in the opensource (free) version. using proxy id=12, retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "AssertionError\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "AssertionError\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "AssertionError\n"
     ]
    }
   ],
   "source": [
    "# Target URL to scrape\n",
    "\n",
    "url = \"https://sgpgrid.com/company-details/\"\n",
    "output_file = \"{}-result.csv\".format(input_file)\n",
    "size = len(df) # Size of dataset\n",
    "\n",
    "start = 79\n",
    "end = start+1 # size\n",
    "\n",
    "max_tries = 15\n",
    "\n",
    "print(\"Beginning job with id={} of size={}\".format(start, size))\n",
    "\n",
    "\n",
    "scraper = cloudscraper.create_scraper()\n",
    "\n",
    "for i in range(start, end):\n",
    "    entity = df.loc[i] # current row record\n",
    "    entity_uen = entity[1] # entity UEN\n",
    "    entity_name = entity[2] # entity name\n",
    "    \n",
    "    entity_url_slug = get_entity_url_slug(entity_name)\n",
    "    \n",
    "    \n",
    "    # Prepare the CSV to write out\n",
    "    formatted_csv_row = \"\\\"{}\\\",\\\"{}\\\",\\\"{}\\\",\\\"{}\\\"\".format(\n",
    "        entity[0],\n",
    "        entity[1],\n",
    "        entity[2],\n",
    "        entity[3]\n",
    "    )\n",
    "\n",
    "    # Accessing the webpage\n",
    "    print(\"Searching for id={} {} {}\".format(i, entity_uen, entity_name))\n",
    "    \n",
    "    for j in range(max_tries):\n",
    "        # Pick a random rotating proxy\n",
    "        id = random.randint(0, proxy_count)\n",
    "        \n",
    "        try:\n",
    "            random_stuff = \"?ra={}&utm_source=facebook&utm_medium=facebook&utm_campaign=\".format(str(random.random()))\n",
    "            url_new = url + entity_url_slug + random_stuff\n",
    "            print(url_new)\n",
    "\n",
    "            # Setup User Agent headers, attempt to imitate a \"browser-like\" request to the webpage\n",
    "#             headers = requests.utils.default_headers()\n",
    "#             headers.update({\n",
    "#                 \"User-Agent\": get_random_user_agent()\n",
    "#             })\n",
    "            \n",
    "            # Now, we query the target URL using a random proxy\n",
    "            resp = scraper.get(url_new, proxies=proxies)\n",
    "            \n",
    "            # Use BeautifulSoup to parse the HTML\n",
    "            soup = bs4.BeautifulSoup(resp.text, \"html.parser\")\n",
    "            print(soup)\n",
    "            \n",
    "            # Now look for the specific elements\n",
    "            columns = soup.find_all(\"div\", {\"class\": \"rt-td table-cell\"})\n",
    "            \n",
    "           \n",
    "            \n",
    "            # Then append SGPGrid's data...\n",
    "#             formatted_csv_row += \",\\\"{}\\\",\\\"{}\\\",\\\"{}\\\"\".format(description, website, ssic)\n",
    "\n",
    "            # Finally we write out to file by appending\n",
    "            f = open(output_file, \"a\")\n",
    "            f.write(formatted_csv_row + \"\\n\")\n",
    "            f.close()\n",
    "            \n",
    "            print(\"    [SUCCESS] Found company info for id={} on retry={}\".format(i, j), description, website, ssic)\n",
    "    \n",
    "            break # next for loop\n",
    "        except Exception as e:\n",
    "            print(\"    [ERROR] Failed at id={} retry={} error={} using proxy id={}, retrying...\".format(i, j, e, id))\n",
    "            \n",
    "            if j == max_tries - 1:\n",
    "                f = open(\"{}-failed.csv\".format(input_file), \"a\")\n",
    "                f.write(formatted_csv_row + \"\\n\")\n",
    "                f.close()\n",
    "            continue # retry\n",
    "\n",
    "    time.sleep(random.randint(1, 8)) # Randomise the waiting time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Codes\n",
    "\n",
    "The following code attempts to:\n",
    "\n",
    "- crawl Google Search for the first 10 results\n",
    "- then it grabs the URLs so that we can crawl them again for the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Target URL to scrape\n",
    "\n",
    "# url = \"https://www.google.com/search?q={}&sourceid=chrome&ie=UTF-8\"\n",
    "# output_file = \"{}-result.csv\".format(input_file)\n",
    "# size = len(df) # Size of dataset\n",
    "\n",
    "# max_tries = 15\n",
    "\n",
    "# for i in range(0, size):\n",
    "#     entity = df.loc[i] # current row record\n",
    "#     entity_uen = entity[1] # entity UEN\n",
    "#     entity_name = entity[2] # entity name\n",
    "    \n",
    "#     # Randomise search terms\n",
    "#     search_terms = [\n",
    "#         \"{} singapore website\".format(entity_name),\n",
    "#         \"{} sg website\".format(entity_name)\n",
    "#     ]\n",
    "    \n",
    "#     search_terms_count = len(search_terms)\n",
    "    \n",
    "#     # Prepare the CSV to write out\n",
    "#     formatted_csv_row = \"\\\"{}\\\",\\\"{}\\\",\\\"{}\\\",\\\"{}\\\"\".format(\n",
    "#         entity[0],\n",
    "#         entity[1],\n",
    "#         entity[2],\n",
    "#         entity[3]\n",
    "#     )\n",
    "\n",
    "#     # Accessing the webpage\n",
    "#     print(\"Searching for id={} {} {}\".format(i, entity_uen, entity_name))\n",
    "    \n",
    "#     for j in range(max_tries):\n",
    "#         try:\n",
    "#             id = random.randint(0, search_terms_count)\n",
    "#             search_term = search_terms[id]\n",
    "#             url_new = url.format(search_term)\n",
    "            \n",
    "#             # Setup User Agent headers, attempt to imitate a \"browser-like\" request to the webpage\n",
    "#             headers = requests.utils.default_headers()\n",
    "#             headers.update({\n",
    "#                 'User-Agent': get_random_user_agent()\n",
    "#             })\n",
    "\n",
    "#             # Pick a random rotating proxy\n",
    "#             id = random.randint(0, proxy_count)\n",
    "            \n",
    "#             # Now, we query the target URL using a random proxy\n",
    "#             resp = requests.get(url_new, headers=headers, proxies=proxies[id])\n",
    "            \n",
    "#             # Use BeautifulSoup to parse the HTML\n",
    "#             soup = bs4.BeautifulSoup(resp.text, \"html.parser\")\n",
    "#             print(soup)\n",
    "            \n",
    "#             # Now look for the specific elements\n",
    "#             headers = soup.find_all(\"h3\")\n",
    "#             links = soup.find_all(href=re.compile(r'\\/url\\?q=')) # pick top 10 search results & its link\n",
    "            \n",
    "#             for i in range(10):\n",
    "#                 google_url = links[i].get(\"href\").strip()\n",
    "#                 parsed_url = parse_qs(google_url)\n",
    "#                 formatted_csv_row += \",{}\".format(parsed_url[\"/url?q\"][0])\n",
    "            \n",
    "#             # Finally we write out to file by appending\n",
    "#             f = open(output_file, \"a\")\n",
    "#             f.write(formatted_csv_row + \"\\n\")\n",
    "#             f.close()\n",
    "            \n",
    "#             print(\"    [SUCCESS] Wrote to file for id={} on retry={}\".format(i, j))\n",
    "    \n",
    "#             break # next for loop\n",
    "#         except Exception as e:\n",
    "#             print(\"    [ERROR] Failed at id={} retry={} error={}, retrying...\".format(i, j, e))\n",
    "            \n",
    "#             if j == max_tries - 1:\n",
    "#                 f = open(\"{}-failed.csv\".format(input_file), \"a\")\n",
    "#                 f.write(formatted_csv_row)\n",
    "#                 f.close()\n",
    "#             continue # retry\n",
    "\n",
    "#     time.sleep(random.randint(1, 3)) # Randomise the waiting time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Target URL to scrape\n",
    "\n",
    "# url = \"http://3.0.205.74/search-results?target={\\\"value\\\":\\\"Registration Number\\\",\\\"label\\\":\\\"Registration Number\\\",\\\"searchTarget\\\":\\\"registrationNumber\\\"}&value=\"\n",
    "# output_file = \"{}-result.csv\".format(input_file)\n",
    "# size = len(df) # Size of dataset\n",
    "\n",
    "# start = 70\n",
    "# print(\"Beginning job with id={} of size={}\".format(start, size))\n",
    "\n",
    "# max_tries = 15\n",
    "\n",
    "# for i in range(start, size):\n",
    "#     entity = df.loc[i] # current row record\n",
    "#     entity_uen = entity[1] # entity UEN\n",
    "#     entity_name = entity[2] # entity name\n",
    "    \n",
    "#     # Prepare the CSV to write out\n",
    "#     formatted_csv_row = \"\\\"{}\\\",\\\"{}\\\",\\\"{}\\\",\\\"{}\\\"\".format(\n",
    "#         entity[0],\n",
    "#         entity[1],\n",
    "#         entity[2],\n",
    "#         entity[3]\n",
    "#     )\n",
    "\n",
    "#     # Accessing the webpage\n",
    "#     print(\"Searching for id={} {} {}\".format(i, entity_uen, entity_name))\n",
    "    \n",
    "#     for j in range(max_tries):\n",
    "#         # Pick a random rotating proxy\n",
    "#         id = random.randint(0, proxy_count)\n",
    "        \n",
    "#         try:\n",
    "#             random_stuff = \"&ra={}&utm_source=facebook&utm_medium=facebook&utm_campaign=\".format(str(random.random()))\n",
    "#             url_new = url + entity_uen + random_stuff\n",
    "\n",
    "#             # Setup User Agent headers, attempt to imitate a \"browser-like\" request to the webpage\n",
    "#             headers = requests.utils.default_headers()\n",
    "#             headers.update({\n",
    "#                 \"User-Agent\": get_random_user_agent()\n",
    "#             })\n",
    "            \n",
    "#             # Now, we query the target URL using a random proxy\n",
    "#             resp = requests.get(url_new, headers=headers, proxies=proxies[id])\n",
    "            \n",
    "#             # Use BeautifulSoup to parse the HTML\n",
    "#             soup = bs4.BeautifulSoup(resp.text, \"html.parser\")\n",
    "#             # print(soup)\n",
    "            \n",
    "#             # Now look for the specific elements\n",
    "#             columns = soup.find_all(\"div\", {\"class\": \"rt-td table-cell\"})\n",
    "            \n",
    "#             description = columns[0].get_text()\n",
    "#             website = columns[3].get_text()\n",
    "#             ssic = columns[5].get_text()\n",
    "            \n",
    "#             # Then append SGPGrid's data...\n",
    "#             formatted_csv_row += \",\\\"{}\\\",\\\"{}\\\",\\\"{}\\\"\".format(description, website, ssic)\n",
    "\n",
    "#             # Finally we write out to file by appending\n",
    "#             f = open(output_file, \"a\")\n",
    "#             f.write(formatted_csv_row + \"\\n\")\n",
    "#             f.close()\n",
    "            \n",
    "#             print(\"    [SUCCESS] Found company info for id={} on retry={}\".format(i, j), description, website, ssic)\n",
    "    \n",
    "#             break # next for loop\n",
    "#         except Exception as e:\n",
    "#             print(\"    [ERROR] Failed at id={} retry={} error={} using proxy id={}, retrying...\".format(i, j, e, id))\n",
    "            \n",
    "#             if j == max_tries - 1:\n",
    "#                 f = open(\"{}-failed.csv\".format(input_file), \"a\")\n",
    "#                 f.write(formatted_csv_row + \"\\n\")\n",
    "#                 f.close()\n",
    "#             continue # retry\n",
    "\n",
    "#     time.sleep(random.randint(1, 8)) # Randomise the waiting time\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
